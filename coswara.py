# -*- coding: utf-8 -*-
"""coswara.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1x5u_uzPXAD2ppEpf7ItAqiSCVi6F1wT0
"""

!nvidia-smi

from google.colab import drive
drive.mount('/content/drive')

# !pip install opensmile
# !pip install mpl_toolkits
# !pip install upsetplot

#!git clone https://github.com/iiscleap/MuDiCov.git

#%cd MuDiCov

#!bash run.sh

# !cp /content/drive/MyDrive/ai/coswara.zip ./coswara.zip
# !unzip /content/coswara.zip

# !wget http://cml12.csie.ntu.edu.tw:1212/coswara.zip

# !cp /content/drive/MyDrive/ai/AI-Final-main.zip ./
# !unzip ./AI-Final-main.zip

# from opensoundscape.audio import Audio
# from opensoundscape.spectrogram import Spectrogram
# from pathlib import Path

# # Settings
# image_shape = (360, 360) #(height, width) not (width, height)
# image_save_path = Path('./saved_spectrogram1.png')

# # Load audio file as Audio object
# audio = Audio.from_file('/content/coswara_normalized/03TmwzsdEBVEh35MRMbC9d0NnfI3/breathing-deep.wav')

# # Create Spectrogram object from Audio object
# spectrogram = Spectrogram.from_audio(audio)

# # Convert Spectrogram object to PIL Image
# image = spectrogram.to_image(shape=image_shape)

# # Save image to file
# image.save(image_save_path)

"""## utils"""

!pip install accelerate
!pip install transformers
!pip install lightgbm
!pip install timm

import torch
import numpy as np
import random
from torch import nn

from random import randint

def same_seeds(seed):
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True

def random_subsample(wav, max_length):
    """Randomly sample chunks of `max_length` seconds from the input audio"""
    # print('called this (', wav.shape, max_length, end = ')-> ')
    n = wav.shape[1]
    if n < max_length: # padding 
        # print("early return => because n =", n, end = ' ')
        wav = torch.cat([wav, torch.zeros(max_length - n).unsqueeze(0)], dim=1)
        return wav
    # random truncation
    random_offset = randint(0, n - max_length - 1)
    # print(random_offset, end = '=> ')
    return wav[:, random_offset : random_offset + max_length]

"""## dataset"""

from random import random
import torch
import torchaudio
import os
import sys

from torch import nn  
from typing import List, Dict
from torch.utils.data import Dataset
from collections import defaultdict
import numpy as np

import torchaudio
from torchaudio import transforms

import re
import torch
import glob

import pandas as pd


def spectro_gram(aud, n_mels=64, n_fft=1024, hop_len=None):

    top_db = 80

    # spec has shape [channel, n_mels, time], where channel is mono, stereo etc
    spec = transforms.MelSpectrogram(16000, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(aud)

    # Convert to decibels
    spec = transforms.AmplitudeToDB(top_db=top_db)(spec)
    return (spec)

def spectro_augment(spec, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1):
    _, n_mels, n_steps = spec.shape
    mask_value = spec.mean()
    aug_spec = spec

    freq_mask_param = max_mask_pct * n_mels
    for _ in range(n_freq_masks):
      aug_spec = transforms.FrequencyMasking(freq_mask_param)(aug_spec, mask_value)

    time_mask_param = max_mask_pct * n_steps
    for _ in range(n_time_masks):
      aug_spec = transforms.TimeMasking(time_mask_param)(aug_spec, mask_value)

    return aug_spec


class CoswaraDataset(Dataset):
    def __init__(
        self,
        train_csv: pd.DataFrame,
        audio_path: str,
        label_mapping: str,
        max_len: int = 160000,
        squeeze = False,
        mode: str = "train",
        return_type = "pt"
    ):
        #df = pd.read_csv(csv_path)
        self.audio_dir = audio_path
        with open(label_mapping) as f:
            self.label_mapping = eval(f.read())
        self._idx2label = {idx: intent for intent, idx in self.label_mapping.items()}
        self.max_len = max_len
        #self.mode = mode
        self.padding = nn.ConstantPad1d(self.max_len, 0.0)
        #self.regression_mapping = [0, 0, 1, 1, 1, 0, 0]
        self.squeeze = squeeze
        #n_samples = 100000
        #df['covid_status'] = df['covid_status'].map(lambda x: self.regression_mapping[x])
        #n_labels = len(set(df['covid_status']))
        #for i in range(n_labels):
        #    n_samples = min(n_samples, len(df[df['covid_status'] == i]))
        #print(f"{n_samples}, {n_labels}")
        #df = df.groupby('covid_status').apply(lambda x: x.sample(n_samples, random_state=42))
        self.data = train_csv
        #self.return_type = return_type
        print(self.data)

    def __len__(self) -> int:
        return len(self.data)

    def __getitem__(self, index) -> Dict:
        instance = self.data.iloc[index]
        ret = { "id": instance.id, "label": instance.covid_status }
        return ret

    @property
    def num_classes(self) -> int:
        return len(self.label_mapping)

    def label2idx(self, label: str):
        return self.label_mapping[label]

    def idx2label(self, idx: int):
        return self._idx2label[idx]

    def collate_fn(self, samples: List[Dict]) -> Dict:
        ret = {'id': [], 'label': [], 'wav': []}
        for sample in samples:
            _id, _label = sample['id'], sample['label']
            root_path = os.path.join(self.audio_dir, _id, "cough-*.wav")
            tmp_wav = []
            for wav_path in glob.glob(root_path):
                wav, sr = torchaudio.load(wav_path)
                if wav.shape[1] < 16000: 
                    break
                # print(wav_path)
                if sr != 16000:
                    downsample = torchaudio.transforms.Resample(sr) # downsample to 16000 Hz
                    wav = downsample(wav)
                # print("original:",wav.shape)


                wav = random_subsample(wav, self.max_len)
                #wav = (wav - all_mean) / all_std
                #wav = wav.squeeze()
                #print(wav)
                sgram = spectro_gram(wav, n_mels=64, n_fft=1024, hop_len=None)
                aug_sgram = spectro_augment(sgram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2)
            
                aug_sgram = torch.tensor(aug_sgram)
                if len(tmp_wav) == 1:
                    tmp_wav[0] = torch.cat((tmp_wav[0], aug_sgram), dim = 1)
                else:
                    tmp_wav.append(aug_sgram)
                # ret['wav'].append(aug_sgram)
                # ret['id'].append(_id)
                # ret['label'].append(_label) # self.regression_mapping[_label])

            if len(tmp_wav) != 0:
                shap = tmp_wav[0].shape

            if len(tmp_wav) != 0 and shap[1] == 128:
                #print("kasdl;fjklskdjkl"
                if len(ret['wav']) == 0:
                    ret['wav'] = tmp_wav[0]
                else:
                    ret['wav'] = torch.cat((ret['wav'], tmp_wav[0]))
                #ret['wav'].append(tmp_wav)
                ret['id'].append(_id)
                ret['label'].append(_label)
        
        ret['wav'] = torch.unsqueeze(ret['wav'], 1)

        ret['label'] = torch.tensor(ret['label']).long()
        #print(ret['label'].shape)
        #ret['wav'] = torch.stack(ret['wav'])
    
        #print(ret['wav'].shape)

        if self.squeeze:
            ret['wav'] = ret['wav'].squeeze(1)
        #if self.return_type == "np":
        #    ret['wav'] = ret['wav'].numpy()
        return ret

"""## model

## m5
"""

import torch
from torch import nn
import torch.nn.functional as F
from torch.nn import init
import timm

def init_weight(module):
    if isinstance(module, (nn.Linear, nn.Conv1d)):
        nn.init.xavier_normal_(module.weight.data)


class MLPBlock(nn.Module):
    def __init__(self, input_dim, output_dim, drop_out=0):
        super(MLPBlock, self).__init__()

        self.block = nn.Sequential(
            nn.Linear(input_dim, output_dim),
            nn.BatchNorm1d(output_dim),
            nn.LeakyReLU(),
            nn.Dropout(drop_out),
        )

    def forward(self, x):
        x = self.block(x)
        return x

class Classifier(nn.Module):
    def __init__(
        self, in_dim, out_dim, hidden_dim=-1, n_layers=1, act_fn="LeakyReLU", **kwargs
    ):
        super(Classifier, self).__init__()
        self.mlp = nn.Linear(in_dim, out_dim)
        if hidden_dim > 0 and n_layers > 1:
            self.mlp = nn.Sequential(
                MLPBlock(in_dim, hidden_dim),
                *[MLPBlock(hidden_dim, hidden_dim) for _ in range(n_layers - 2)],
                nn.Linear(hidden_dim, out_dim),
            )
        self.act_fn = None
        if act_fn != "None":
            self.act_fn = getattr(nn, act_fn)(**kwargs)

    def forward(self, x):
        x = self.mlp(x)
        if self.act_fn is not None:
            x = self.act_fn(x)
        return x

class ConvBlock(nn.Module):
    def __init__(self, in_dim, out_dim, kernel_size, stride=1, padding=0, p=0.3):
        super(ConvBlock, self).__init__()

        self.block = nn.Sequential(
            nn.Conv1d(in_dim, out_dim, kernel_size, stride, padding),
            nn.BatchNorm1d(out_dim),
            nn.ReLU(),
            nn.Dropout(p),
        )

    def forward(self, inputs):
        return self.block(inputs)


class M5(nn.Module):
    def __init__(
        self,
        n_input=1,
        n_output=2,
        stride=4,
        n_channel=128,
        hidden_dim=-1,
        n_layers=1,
        act_fn="LeakyReLU",
        **kwargs
    ):
        super().__init__()
        self.feature_extractor = nn.Sequential(
            ConvBlock(n_input, n_channel, 80, stride=stride, p=0),
            nn.MaxPool1d(4),
            ConvBlock(n_channel, n_channel, 3, p=0),
            nn.MaxPool1d(4),
            ConvBlock(n_channel, 2 * n_channel, 3, p=0),
            nn.MaxPool1d(4),
            ConvBlock(2 * n_channel, 4 * n_channel, 3, p=0),
            nn.MaxPool1d(4),
        )

        self.cls = Classifier(
            4 * n_channel,
            n_output,
            hidden_dim=hidden_dim,
            n_layers=n_layers,
            act_fn=act_fn,
            **kwargs,
        )
        self.apply(init_weight)

    def forward(self, x):
        x = self.feature_extractor(x)
        x = F.avg_pool1d(x, x.shape[-1])
        x = x.squeeze(-1)
        x = self.cls(x)
        return x

    def get_embedding(self, x):
        x = self.feature_extractor(x)
        x = F.avg_pool1d(x, x.shape[-1])
        x = x.squeeze(-1)
        return x

"""## CNN"""

import torch
from torch import nn
import torch.nn.functional as F
from torch.nn import init

class AudioClassifier (nn.Module):
    # ----------------------------
    # Build the model architecture
    # ----------------------------
    def __init__(self):
        super().__init__()
        conv_layers = []

        # First Convolution Block with Relu and Batch Norm. Use Kaiming Initialization
        self.conv1 = nn.Conv2d(2, 4, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))
        self.relu1 = nn.ReLU()
        self.bn1 = nn.BatchNorm2d(4)
        init.kaiming_normal_(self.conv1.weight, a=0.1)
        self.conv1.bias.data.zero_()
        self.do1 = nn.Dropout(0.2)
        conv_layers += [self.conv1, self.relu1, self.bn1, self.do1]

        # Second Convolution Block
        self.conv2 = nn.Conv2d(4, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        self.relu2 = nn.ReLU()
        self.bn2 = nn.BatchNorm2d(16)
        init.kaiming_normal_(self.conv2.weight, a=0.1)
        self.conv2.bias.data.zero_()
        self.do2 = nn.Dropout(0.2)
        conv_layers += [self.conv2, self.relu2, self.bn2, self.do2]

        # Second Convolution Block
        self.conv3 = nn.Conv2d(16, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        self.relu3 = nn.ReLU()
        self.bn3 = nn.BatchNorm2d(64)
        init.kaiming_normal_(self.conv3.weight, a=0.1)
        self.conv3.bias.data.zero_()
        self.do3 = nn.Dropout(0.2)
        conv_layers += [self.conv3, self.relu3, self.bn3, self.do3]

        # Second Convolution Block
        self.conv4 = nn.Conv2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        self.relu4 = nn.ReLU()
        self.bn4 = nn.BatchNorm2d(32)
        init.kaiming_normal_(self.conv4.weight, a=0.1)
        self.conv4.bias.data.zero_()
        conv_layers += [self.conv4, self.relu4, self.bn4, self.do4]

        # Linear Classifier
        self.ap = nn.AdaptiveAvgPool2d(output_size=4)
        self.lin = nn.Linear(in_features=512, out_features=2)

        # Wrap the Convolutional Blocks
        self.conv = nn.Sequential(*conv_layers)
 
    # ----------------------------
    # Forward pass computations
    # ----------------------------
    def forward(self, x):
        # Run the convolutional blocks
        x = self.conv(x)
        #print(x.shape)
        # Adaptive pool and flatten for input to linear layer
        x = self.ap(x)
        #print(x.shape)
        x = x.view(x.shape[0], -1)
        #print(x.shape)
        # Linear layer
        x = self.lin(x)
        #print(x.shape)
        # Final output
        return x

"""## Train"""

import os
from argparse import ArgumentParser, Namespace
from pathlib import Path

import torch
import numpy as np
from accelerate import Accelerator
from torch import nn
from torch.utils.data import DataLoader
from tqdm import tqdm
from transformers import (
    get_cosine_schedule_with_warmup,
)
import sys
from sklearn.metrics import roc_auc_score
from xgboost import XGBClassifier
import lightgbm as lgbm
from sklearn.model_selection import train_test_split

N_SOUND = 9

np.set_printoptions(threshold=sys.maxsize)

params = {
    'learning_rate' : 0.1,
    'n_estimators' : 128,
    'num_leaves' : 32,
    'max_depth' : 16
}

def inference_validate(data_loader, model):
    model.eval()
    valid_loss = []
    valid_accs = []
    all_labels, all_embeddings = [], []

    for idx, batch in enumerate(tqdm(data_loader)):
        logits = model.get_embedding(batch['wav'])
        #print(batch['wav'])
        labels = batch['label']

        all_labels.extend(labels.cpu().numpy())
        if not len(all_embeddings):
            all_embeddings = logits.detach().cpu().numpy()
        else:
            all_embeddings = np.concatenate([all_embeddings, logits.detach().cpu().numpy()])

    all_labels = np.array(all_labels)
    return all_embeddings, all_labels

def train(accelerator, args, data_loader, model, optimizer, criterion, scheduler=None):
    train_loss = []
    train_accs = []
    log_loss = []
    log_accs = []
    all_labels, all_logits = [], []

    model.train()

    for idx, batch in enumerate(tqdm(data_loader)):
        logits = model(batch['wav'])
        labels = batch['label']
        loss = criterion(logits, labels)
        # loss = criterion(logits, torch.eye(2)[labels].to(accelerator.device))
        acc = (logits.argmax(dim=-1) == labels).cpu().float().mean()
        loss = loss / args.accu_step
        accelerator.backward(loss)

        all_labels.extend(labels.cpu().numpy())
        if not len(all_logits):
            all_logits = logits.detach().cpu().numpy()
        else:
            all_logits = np.concatenate([all_logits, logits.detach().cpu().numpy()])
        train_loss.append(loss.item())
        train_accs.append(acc)
        log_loss.append(loss.item())
        log_accs.append(acc)

        if ((idx + 1) % args.accu_step == 0) or (idx == len(data_loader) - 1):
            optimizer.step()
            if scheduler is not None:
                scheduler.step()
            optimizer.zero_grad()

        if ((idx + 1) % args.log_step == 0) or (idx == len(data_loader) - 1):
            log_loss = sum(log_loss) / len(log_loss)
            log_acc = sum(log_accs) / len(log_accs)
            print(f"Train Loss: {log_loss:.4f}, Train Acc: {log_acc:.4f}")
            log_loss, log_acc= [], []
   
    
    train_loss = sum(train_loss) / len(train_loss)
    train_acc = sum(train_accs) / len(train_accs)
    all_labels = np.array(all_labels)
    pred = all_logits.argmax(axis=-1)
    try:
        train_auc = roc_auc_score(np.eye(2)[all_labels], all_logits)
        sensitivity = np.sum((pred == all_labels) & (all_labels == 1)) / np.sum(all_labels == 1)
        specificity = np.sum((pred == all_labels) & (all_labels == 0)) / np.sum(all_labels == 0)
        print(f"----- Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}")
    except Exception as e:
        print(e)

        print(np.sum(all_labels), np.sum(logits.argmax(axis=-1)))
        print(np.sum(1 - all_labels), np.sum(1 - logits.argmax(axis=-1)))
        return train_loss, train_acc, 0

    #embeddings, labels = inference_validate(data_loader, model)
    #clf = lgbm.LGBMClassifier(**params)
    #clf = XGBClassifier(objective="binary:logistic", random_state=1126)
    #clf.fit(embeddings, labels)
    #print("* Train result:", clf.score(embeddings, labels), "auc", roc_auc_score(np.eye(3)[labels], clf.predict_proba(embeddings)))


    return train_loss, train_acc, train_auc



@torch.no_grad()
def validate(accelerator, data_loader, model, criterion, clf=None):
    model.eval()
    valid_loss = []
    valid_accs = []
    all_labels, all_logits = [], []

    #embeddings, labels = inference_validate(data_loader, model)

    #print("* Valid result:", clf.score(embeddings, labels), "auc", roc_auc_score(np.eye(3)[labels], clf.predict_proba(embeddings)))


    for idx, batch in enumerate(tqdm(data_loader)):
        logits = model(batch['wav'])
        labels = batch['label']
        loss = criterion(logits, labels)
        # loss = criterion(logits, torch.eye(2)[labels].to(accelerator.device))
        acc = (logits.argmax(dim=-1) == labels).cpu().float().mean()

        all_labels.extend(labels.cpu().numpy())
        if not len(all_logits):
            all_logits = logits.detach().cpu().numpy()
        else:
            all_logits = np.concatenate([all_logits, logits.detach().cpu().numpy()])
        valid_loss.append(loss.item())
        valid_accs.append(acc)

    all_labels = np.array(all_labels)
    with open('logfile.log', 'w') as f:
        print('all_labels', file=f)
        print(all_labels, file=f)
        print('all_logits', file=f)
        print(all_logits, file=f)
    valid_loss = sum(valid_loss) / len(valid_loss)
    valid_acc = sum(valid_accs) / len(valid_accs)
    pred = all_logits.argmax(axis=-1)
    try:
        valid_auc = roc_auc_score(all_labels, all_logits[:,1])
        sensitivity = np.sum((pred == all_labels) & (all_labels == 1)) / np.sum(all_labels == 1)
        specificity = np.sum((pred == all_labels) & (all_labels == 0)) / np.sum(all_labels == 0)
        print(f"***** Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}")
    except Exception as e:
        print(e)
        print(all_labels, all_logits)
        return valid_loss, valid_acc, 0
    
    return valid_loss, valid_acc, valid_auc


all_mean = -1.0131372e-05
all_std = 0.06298088

def main(args):

    same_seeds(args.seed)
    accelerator = Accelerator()

    #model = M5(1,2, act_fn="Softmax", dim=1)
    #model = AudioClassifier()
    model = timm.create_model(
                'resnet34d', num_classes=2, pretrained=True, in_chans=1)
    print(model)
    optimizer = torch.optim.AdamW(
        model.parameters(), lr=args.lr, weight_decay=args.wd
    )
    criterion = nn.CrossEntropyLoss()

    starting_epoch = 1

    # dataset = CoswaraDataset(
    #     csv_path = os.path.join(args.data_dir, "combined_data.csv"),
    #     audio_path = args.data_dir,
    #     label_mapping = os.path.join(args.data_dir, "mapping.json"),
    #     max_len = args.max_len
    # )

    #train_size = int(round(len(dataset) * 0.8))
    #valid_size = len(dataset) - train_size
    #train_set, valid_set = torch.utils.data.random_split(dataset, [train_size, valid_size], generator=torch.Generator().manual_seed(42))

    csv_path = os.path.join(args.data_dir, "combined_data.csv")
    df = pd.read_csv(csv_path)
    label_mapping_path = os.path.join(args.data_dir, "mapping.json")
    with open(label_mapping_path) as f:
        label_mapping = eval(f.read())
    _idx2label = {idx: intent for intent, idx in label_mapping.items()}
    regression_mapping = [0, 0, 1, 1, 1, 0, 0]
    n_samples = 100000
    df['covid_status'] = df['covid_status'].map(lambda x: regression_mapping[x])
    n_labels = len(set(df['covid_status']))

    train_df, valid_df, _, _ = train_test_split(df,
                                                df['covid_status'],
                                                stratify=df['covid_status'],
                                                test_size=0.2,
                                                random_state=30)

    tmp_df = train_df.loc[train_df['covid_status'] == 1]
    train_df = pd.concat([train_df, tmp_df], ignore_index = True)

    train_set = CoswaraDataset(
        train_csv = train_df,
        audio_path = args.data_dir,
        label_mapping = os.path.join(args.data_dir, "mapping.json"),
        max_len = args.max_len
    )

    valid_set = CoswaraDataset(
        train_csv = valid_df,
        audio_path = args.data_dir,
        label_mapping = os.path.join(args.data_dir, "mapping.json"),
        max_len = args.max_len
    )


    train_loader = DataLoader(
        train_set,
        collate_fn=train_set.collate_fn,
        shuffle=True,
        batch_size=args.batch_size,
        pin_memory=False
    )
    valid_loader = DataLoader(
        valid_set,
        collate_fn=valid_set.collate_fn,
        shuffle=False,
        batch_size=args.batch_size,
        pin_memory=False
    )
    warmup_step = int(0.1 * len(train_loader)) // args.accu_step
    scheduler = None
    # scheduler = get_cosine_schedule_with_warmup(
    #     optimizer, warmup_step, args.num_epoch * len(train_loader) - warmup_step
    # )

    model, optimizer, train_loader, valid_loader = accelerator.prepare(
        model, optimizer, train_loader, valid_loader
    )
    best_loss = float("inf")

    to_train, to_validate = True, True
    for epoch in range(starting_epoch, args.num_epoch + 1):
        print(f"Epoch {epoch}:")
        if to_train:
            train_loss, train_acc, train_auc = train(
                accelerator, args, train_loader, model, optimizer, criterion, scheduler
            )
            print(f" Train Loss: {train_loss:.2f}, Train Accuracy: {train_acc:.2f}, Train AUC: {train_auc:.2f}")
        if to_validate:
            valid_loss, valid_acc, valid_auc = validate(accelerator, valid_loader, model, criterion)
            print(f"Valid Loss: {valid_loss:.2f}, Valid Accuracy: {valid_acc:.2f}, Valid AUC: {valid_auc:.2f}")

        if valid_loss < best_loss:
            best_loss = valid_loss
            torch.save(
                {
                    "epoch": epoch,
                    "model": model.state_dict(),
                    "optimizer": optimizer.state_dict(),
                },
                os.path.join(args.ckpt_dir, f"{args.prefix}_best.ckpt"),
            )
        torch.save(
            {
                "epoch": epoch,
                "model": model.state_dict(),
                "optimizer": optimizer.state_dict(),
            },
            os.path.join(args.ckpt_dir, f"{args.prefix}_latest.ckpt"),
        )


def parse_args():
    parser = ArgumentParser()
    parser.add_argument('-f')
    parser.add_argument("--seed", type=int, default=32)
    parser.add_argument(
        "--data_dir",
        type=Path,
        help="Directory to the dataset.",
        default="./coswara",
    )
    parser.add_argument(
        "--ckpt_dir",
        type=Path,
        help="Directory to save the model file.",
        default="./ckpt/",
    )

    # data
    parser.add_argument("--max_len", type=int, default=96000)

    # optimizer
    parser.add_argument("--lr", type=float, default=2e-3)
    parser.add_argument("--wd", type=float, default=1e-4)

    # data loader
    parser.add_argument("--batch_size", type=int, default=32)

    # training
    parser.add_argument("--num_epoch", type=int, default=60)
    parser.add_argument("--accu_step", type=int, default=1)
    parser.add_argument("--prefix", type=str, default="")
    parser.add_argument("--wandb", action="store_true")
    parser.add_argument("--log_step", type=int, default=10)
    args = parser.parse_args()
    return args


args = parse_args()
args.ckpt_dir.mkdir(parents=True, exist_ok=True)
print(args)

main(args)

assert False

args = parse_args()
dataset = CoswaraDataset(
    csv_path = os.path.join(args.data_dir, "combined_data.csv"),
    audio_path = args.data_dir,
    label_mapping = os.path.join(args.data_dir, "mapping.json"),
)

train_loader = DataLoader(
    dataset,
    collate_fn=dataset.collate_fn,
    shuffle=True,
    batch_size=args.batch_size,
    pin_memory=True
)

value = []
for i, batch in enumerate(tqdm(train_loader)):
    value.extend(np.array(batch['wav'].detach().cpu().flatten()))

value = np.array(value)
value = value.flatten()
print(np.mean(value))
print(np.std(value))
print(np.max(value))
print(np.min(value))

import gc
gc.collect()